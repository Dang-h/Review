# 使用Taildir Source从不同节点采集日志使用Kafka Channel导入HDFS
# 接收101和102传过来的数据，通过kafkaChannel再导入HDFS
# a1为agent别名，r1为source别名，c1为channel别名，k1为sink别名
# 使用拦截器给每条日志增加日志采集时间

a1.sources = r1
a1.sinks = k1
a1.channels = c1


a1.sources.r1.type = avro
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 4545
#a1.sources.r1.interceptors = i1
# flume自带timestamp拦截器，在event的header中添加timestamp
#a1.sources.r1.interceptors.i1.type = timestamp

a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop100:9092,hadoop101:9092，hadoop102:9092
a1.channels.c1.kafka.topic = first
a1.channels.c1.parseAsFlumeEvent = false

a1.sources.r1.channels = c1
