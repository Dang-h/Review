# 使用Taildir Source从不同节点采集日志使用Kafka Channel导入HDFS
# 接收101和102传过来的数据，通过kafkaChannel再导入HDFS
# a1为agent别名，r1为source别名，c1为channel别名，k1为sink别名
# 使用拦截器给每条日志增加日志采集时间

a1.sources = r1
a1.sinks = k1
a1.channels = c1


a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop100:9092,hadoop101:9092,hadoop102:9092
a1.sources.r1.kafka.topics= first
a1.sources.r1.interceptors = i1
# 增加时间戳拦截器
a1.sources.r1.interceptors.i1.type =interceptor.TimeAddInterceptor$Builder

a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume-1.9.0/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/flume-1.9.0/data/behavior1/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
# 单位s
a1.channels.c1.keep-alive = 6


a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path =hdfs://hadoop100:9820/flumeTest/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = TEXT
# 是否按照时间滚动文件夹
a1.sinks.k1.hdfs.round = false
#多少时间单位创建一个新的文件夹
#a2.sinks.k2.hdfs.roundValue = 5
#重新定义时间单位
#a2.sinks.k2.hdfs.roundUnit = second
#多久生成一个新的文件10s
a1.sinks.k1.hdfs.rollInterval = 10
#设置每个文件的滚动大小(Byte)
a1.sinks.k1.hdfs.rollSize = 10
#文件的滚动与 Event 数量无关
a1.sinks.k1.hdfs.rollCount = 0
#a1.sinks.k1.hdfs.useLocalTimeStamp = true

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1