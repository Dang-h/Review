# 使用Taildir Source从不同节点采集日志使用Kafka Channel导入HDFS
# 将101和102中的日志传输到100
# a1为agent别名，r1为source别名，c1为channel别名，k1为sink别名

a1.sources = r1
a1.channels = c1
a1.sinks = k1

# source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*
a1.sources.r1.positionFile =/opt/module/flume-1.9.0/checkpoint/positionFile/taildir_position.json


# channel
a1.channels.c1.type = file
a1.channels.c1.checkpointDir =/opt/module/flume-1.9.0/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/flume-1.9.0/data/behavior1/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 6

# sink
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = hadoop100
a1.sinks.k1.port = 4545

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1